---
title: "Mini Project 3"
author: "Grace Wagner and Nathanael Kovscek"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Front Matter
```{r warnings = FALSE, message = FALSE}
remove(list = ls())
# Load necessary libraries
library(tidyverse)
library(ggplot2)
library(lubridate)

#Part 1
library(readxl)
CODGames2_mp <- read_excel("data/CODGames2_mp.xlsx")
```

## Task 1

First, we would like to see the distribution of Full and Partial games. That way, we are able to see what the dimensions of the filtered data table should have.
```{r}
CODGames2_mp %>% 
  group_by(FullPartial) %>%
  summarise(n = n())
```

Given this output, when we filter out Partial values from the data table, we should have 191 observations.

```{r}
full_games <- CODGames2_mp %>%
  filter(FullPartial == "Full")
```

Now that this is done, let's see what the distribution of TotalXP is in Full Games. We will visualize this distribution using two boxplots where each is one of the two XPTypes that it is possible for the player to obtain.

```{r}
ggplot(full_games, mapping = aes(x = XPType, y = TotalXP)) +
  geom_boxplot()

```

This shown, it is much more favorable for the user to have the Double XP + 10% XPType than the 10% Boost type. The user's totalXP median earning is approximately 6000 more than it otherwise would be. Additionally, the interquartile range for Double XP + 10% is larger and there are more outliers on the high end than there is for 10% Boost type.

Out of curiosity, we would like to see the distribution of the XPTypes.

```{r}
CODGames2_mp %>% 
  group_by(XPType) %>%
  summarise(n = n())
```

I'm not sure if this is more or less surprising given what we know about the game, but my intuition suggests that something which increases the amount of earned XP by a median average of around 6000 would be more rare in the game. Additionally, even though there are fewer uses of the DoubleXP, we would still expect the larger IQR because the XP earned is double that which was received. Looking at the boxplots again, I would suggest that the IQR for the 10% Boost is half the IQR for the DoubleXP type!


## Task 2

```{r}
tdm <- full_games %>% 
  filter(GameType == "HC - TDM") %>% 
  separate(Result, into = c("PlayersTeam", "OtherTeam"), sep = "-", convert = TRUE)
tdm <- tdm %>% 
  mutate(GameResult = tdm$PlayersTeam - tdm$OtherTeam) %>%
  mutate(GameResult2 = case_when(GameResult > 0 ~ "win", GameResult < 0 ~ "lose", GameResult == 0 ~ "tie")) %>%
  mutate(Win = ifelse(GameResult2 == "win", 1, 0)) %>% 
  select(TotalXP, Eliminations, Deaths, Damage, XPType, Win, Score)
```

```{r}
#set seed
set.seed(1)

#form needed for glmnet
Xmat <- model.matrix(Score ~ . , data = tdm)[ ,-1]  
y <- tdm$Score

#no train test split
#fit model
lasso.mod <- glmnet(x = Xmat, y = y, 
                    alpha = 1,
                    standardize = TRUE)

#create plot of coefficients
plot(lasso.mod, xvar="lambda",label=TRUE)
```
```{r}
#plot
#set seed
set.seed(1)

#cv for k-fold cross validation 
cv.out <- cv.glmnet(x = Xmat, y = y, 
                    alpha = 1, standardize = TRUE,
                    nfolds = 10)
plot(cv.out)
```

```{r}
#set seed
set.seed(1)

#min lambda
bestlam1 <- cv.out$lambda.min
#predict responses
lasso.pred1 <- predict(cv.out, s = bestlam1,
                       newx = Xmat)
#find coefficients
lasso.coef1 <- predict(cv.out, s = bestlam1,
                       type = "coefficients")

bestlam1
lasso.coef1

#1 standard error lambda
bestlam2 <- cv.out$lambda.1se
#predict responses
lasso.pred2 <- predict(cv.out, s = bestlam2,
                       newx = Xmat)
#find coefficients
lasso.coef2 <- predict(cv.out, s = bestlam2,
                       type = "coefficients")
bestlam2
lasso.coef2

```
NEED TO CHANGE THIS SO NOT COMPARING RMSE
The value of lambda that I chose was 2.379934, the min lambda. This lambda had a smaller RMSE value than the lambda found using 1se. It had an RMSE of 692.8955 which is smaller than 838.7051, which is good since we are looking for the smallest RMSE. 

```{r}
#set.seed
set.seed(1)

#feature selection 2 - Backwards Elimination 
int_only_model <- glm(Score ~ 1, family = gaussian, data = tdm)

full_model <- glm(Score ~ . -Score, family = gaussian, data = tdm)

stats::step(object = full_model, 
            scope = list(lower = int_only_model, upper = full_model),
            data = tdm,
            direction = "backward")

regression_model <- lm(Score ~ TotalXP + Eliminations + Deaths + XPType + 
    Win, data = tdm)
regression_model
#equation includes, TotalXP + Eliminations + Deaths + XPType + Win
```
