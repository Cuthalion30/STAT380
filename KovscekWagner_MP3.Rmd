---
title: "Mini Project 3"
author: "Grace Wagner and Nathanael Kovscek"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Front Matter
```{r warnings = FALSE, message = FALSE}
remove(list = ls())
# Load necessary libraries
library(tidyverse)
library(ggplot2)
library(lubridate)
library(rpart)
library(rattle)
library(glmnet)
#Part 1
library(readxl)
CODGames2_mp <- read_excel("C:/Users/Grace Wagner/Downloads/CODGames2_mp.xlsx")
```

## Task 1

First, we would like to see the distribution of Full and Partial games. That way, we are able to see what the dimensions of the filtered data table should have.
```{r}
CODGames2_mp %>% 
  group_by(FullPartial) %>%
  summarise(n = n())
```

Given this output, when we filter out Partial values from the data table, we should have 191 observations.

```{r}
full_games <- CODGames2_mp %>%
  filter(FullPartial == "Full")
```

Now that this is done, let's see what the distribution of TotalXP is in Full Games. We will visualize this distribution using two boxplots where each is one of the two XPTypes that it is possible for the player to obtain.

```{r}
ggplot(full_games, mapping = aes(x = XPType, y = TotalXP)) +
  geom_boxplot()
```

This shown, it is much more favorable for the user to have the Double XP + 10% XPType than the 10% Boost type. The user's totalXP median earning is approximately 6000 more than it otherwise would be. Additionally, the interquartile range for Double XP + 10% is larger and there are more outliers on the high end than there is for 10% Boost type.

Out of curiosity, we would like to see the distribution of the XPTypes.

```{r}
CODGames2_mp %>% 
  group_by(XPType) %>%
  summarise(n = n())
```

I'm not sure if this is more or less surprising given what we know about the game, but my intuition suggests that something which increases the amount of earned XP by a median average of around 6000 would be more rare in the game. Additionally, even though there are fewer uses of the DoubleXP, we would still expect the larger IQR because the XP earned is double that which was received. Looking at the boxplots again, I would suggest that the IQR for the 10% Boost is half the IQR for the DoubleXP type!


## Task 2

```{r}
tdm <- full_games %>% 
  filter(GameType == "HC - TDM") %>% 
  separate(Result, into = c("PlayersTeam", "OtherTeam"), sep = "-", convert = TRUE)
tdm <- tdm %>% 
  mutate(GameResult = tdm$PlayersTeam - tdm$OtherTeam) %>%
  mutate(GameResult2 = case_when(GameResult > 0 ~ "win", GameResult < 0 ~ "lose", GameResult == 0 ~ "tie")) %>%
  mutate(Win = ifelse(GameResult2 == "win", 1, 0)) %>% 
  select(TotalXP, Eliminations, Deaths, Damage, XPType, Win, Score)
```

##2a
```{r}
#set seed
set.seed(1)

#form needed for glmnet
Xmat <- model.matrix(Score ~ . , data = tdm)[ ,-1]  
y <- tdm$Score

#no train test split
#fit model
lasso.mod <- glmnet(x = Xmat, y = y, 
                    alpha = 1,
                    standardize = TRUE)

#create plot of coefficients
plot(lasso.mod, xvar="lambda",label=TRUE)
```
```{r}
#plot
#set seed
set.seed(1)

#cv for k-fold cross validation 
cv.out <- cv.glmnet(x = Xmat, y = y, 
                    alpha = 1, standardize = TRUE,
                    nfolds = 10)
plot(cv.out)
```

```{r}
#set seed
set.seed(1)

#min lambda
bestlam1 <- cv.out$lambda.min
#predict responses
lasso.pred1 <- predict(cv.out, s = bestlam1,
                       newx = Xmat)
#find coefficients
lasso.coef1 <- predict(cv.out, s = bestlam1,
                       type = "coefficients")

bestlam1
lasso.coef1

#1 standard error lambda
bestlam2 <- cv.out$lambda.1se
#predict responses
lasso.pred2 <- predict(cv.out, s = bestlam2,
                       newx = Xmat)
#find coefficients
lasso.coef2 <- predict(cv.out, s = bestlam2,
                       type = "coefficients")
bestlam2
lasso.coef2
```

Discussion on which Value of Lambda was Selected:

The value of lambda that we chose was 300.3136, the 1se lambda. The minimum lambda did not get rid of any variables. Since the 1se lambda reduced the feature space, we chose that one. 

The Estimated Equation for LASSO:

$$\hat{y}_i = 5.057312e+02 + 2.323181e-03 x_{i,1} + 1.630272e+02 x_{i,2}$$

```{r}
#set.seed
set.seed(1)

#feature selection 2 - Backwards Elimination 
int_only_model <- glm(Score ~ 1, family = gaussian, data = tdm)

full_model <- glm(Score ~ . -Score, family = gaussian, data = tdm)

stats::step(object = full_model, 
            scope = list(lower = int_only_model, upper = full_model),
            data = tdm,
            direction = "backward")

regression_model <- lm(Score ~ TotalXP + Eliminations + Deaths + XPType + 
    Win, data = tdm)
regression_model
```

The Estimated Equation for Backwards Elimination: 

$$\hat{y}_i = 944.05302 + 0.06054 x_{i,1} + 185.24081 x_{i,2} - 73.25104 x_{i,3} - 367.94000 x_{i, 4} - 454.44701 x_{i, 5}$$

Comparison of the results of LASSO with the results of Backwards Elimination:

The difference between the two results is that the LASSO results based on the minimum lambda value didn't remove any of the variables. The backwards elimination got rid of the Damage variable from the equation, unlike LASSO. 

##2b
```{r}
res <- rpart(Score ~ TotalXP + Eliminations + Deaths + Damage + XPType + Win, method="anova", data=tdm,
             minbucket=15)
#Create plot using fancyRpartPlot for rattle library
fancyRpartPlot(res, cex=0.8)
```
The variables with the three highest importance values are Damage (`r res[["variable.importance"]][["Damage"]]`), Eliminations (`r res[["variable.importance"]][["Eliminations"]]`), and TotalXP (`r res[["variable.importance"]][["TotalXP"]]`)

##2c
```{r}
#Create Indictor(s) for XPType
tdm2 <- tdm %>%
        mutate(XPType = ifelse(XPType == "10% Boost", 1, 0))

#Scale numeric variables
xvars <- c("TotalXP", "Eliminations", "Deaths", "Damage", "XPType", "Win")
tdm2[ , xvars] <- scale(tdm2[ , xvars], center = TRUE, scale = TRUE)

#regression model based on the variables selected by backwards elimination in part a
regression_model2 <- lm(formula = Score ~ TotalXP + Eliminations + Deaths + XPType + Win, data = tdm2)
regression_model2
```

The estimated equation is:


$$\hat{y}_i = 3104.9 + 382.9 x_{i,1} + 1030.9 x_{i,2} - 276.5 x_{i,3} + 182.8 x_{i,4} - 226.1 x_{i,5}$$


3 Most Important variables based on Magnitude of Estimated Coefficients:

Eliminations, TotalXP, and Deaths

How Does this Compare to the Regression Tree:

Damage, Eliminations, and TotalXP were the three most important variables from the regression tree, but the linear regression model based on backwards elimination showed Deaths, Eliminations, and TotalXP as the most important variables. The only difference is that the regression tree included Damage, but the regression model based on backwards elimination included Deaths instead. 

## Discussion of Group Participation

In this project, Nathanael and I did an even split of the work. He did task 1 and part b in task 2 and I did part a and c in task 2. To do this effectively, we met at several points in the process to discuss the questions and the distribution of work. We both found this distribution of work to be effective and would do something similar in the future. 
